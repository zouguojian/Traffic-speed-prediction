import torch
import numpy as np
import torch.utils.data
from lib.add_window import Add_Window_Horizon
from lib.load_dataset import load_st_dataset
from lib.normalization import NScaler, MinMax01Scaler, MinMax11Scaler, StandardScaler, ColumnMinMaxScaler

def normalize_dataset(data, normalizer, column_wise=False):
    if normalizer == 'max01':
        if column_wise:
            minimum = data.min(axis=0, keepdims=True)
            maximum = data.max(axis=0, keepdims=True)
        else:
            minimum = data.min()
            maximum = data.max()
        scaler = MinMax01Scaler(minimum, maximum)
        data = scaler.transform(data)
        print('Normalize the dataset by MinMax01 Normalization')
    elif normalizer == 'max11':
        if column_wise:
            minimum = data.min(axis=0, keepdims=True)
            maximum = data.max(axis=0, keepdims=True)
        else:
            minimum = data.min()
            maximum = data.max()
        scaler = MinMax11Scaler(minimum, maximum)
        data = scaler.transform(data)
        print('Normalize the dataset by MinMax11 Normalization')
    elif normalizer == 'std':
        if column_wise:
            mean = data.mean(axis=0, keepdims=True)
            std = data.std(axis=0, keepdims=True)
        else:
            mean = data.mean()
            std = data.std()
        scaler = StandardScaler(mean, std)
        data = scaler.transform(data)
        print('Normalize the dataset by Standard Normalization')
    elif normalizer == 'None':
        scaler = NScaler()
        data = scaler.transform(data)
        print('Does not normalize the dataset')
    elif normalizer == 'cmax':
        #column min max, to be depressed
        #note: axis must be the spatial dimension, please check !
        scaler = ColumnMinMaxScaler(data.min(axis=0), data.max(axis=0))
        data = scaler.transform(data)
        print('Normalize the dataset by Column Min-Max Normalization')
    else:
        raise ValueError
    return data, scaler

def split_data_by_days(data, val_days, test_days, interval=60):
    '''
    :param data: [B, *]
    :param val_days:
    :param test_days:
    :param interval: interval (15, 30, 60) minutes
    :return:
    '''
    T = int((24*60)/interval)
    test_data = data[-T*test_days:]
    val_data = data[-T*(test_days + val_days): -T*test_days]
    train_data = data[:-T*(test_days + val_days)]
    return train_data, val_data, test_data

def split_data_by_ratio(data, val_ratio, test_ratio):
    data_len = data.shape[0]
    test_data = data[-int(data_len*test_ratio):]
    val_data = data[-int(data_len*(test_ratio+val_ratio)):-int(data_len*test_ratio)]
    train_data = data[:-int(data_len*(test_ratio+val_ratio))]
    return train_data, val_data, test_data

def data_loader(X, Y, batch_size, shuffle=True, drop_last=True):
    cuda = True if torch.cuda.is_available() else False
    TensorFloat = torch.cuda.FloatTensor if cuda else torch.FloatTensor
    X, Y = TensorFloat(X), TensorFloat(Y)
    data = torch.utils.data.TensorDataset(X, Y)
    dataloader = torch.utils.data.DataLoader(data, batch_size=batch_size,
                                             shuffle=shuffle, drop_last=drop_last)
    return dataloader


def generate_data(dataset_dir):
    data = {}
    mean = None
    std = None
    for category in ['train', 'val', 'test']:
        cat_data = np.load(os.path.join(dataset_dir, category + '.npz'))
        data['x_' + category] = np.transpose(cat_data['x'][..., :1], (0, 2, 3, 1))
        data['y_' + category] = np.transpose(cat_data['y'][..., 0], (0, 2, 1))
        # x = cat_data['x'][..., :1]
        # y = cat_data['y'][..., :1]
        if mean is None:
            mean = data['x_train'].mean()
        if std is None:
            std = data['x_train'].std()
        # yield (x - mean) / std, y
    # return (data['x_train'] - mean) / std, (data['y_train'] - mean) / std, (data['x_val'] - mean) / std, (data['y_val'] - mean) / std, (data['x_test'] - mean) / std, (data['y_test'] - mean) / std, mean, std
    return (data['x_train'] - mean) / std, data['y_train'], (data['x_val'] - mean) / std, data['y_val'], (data['x_test'] - mean) / std, data['y_test'], mean, std
    # scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())
    # # Data format
    # for category in ['train', 'val', 'test']:
    #     data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])
    #
    # data['train_loader'] = DataLoaderM(data['x_train'], data['y_train'], batch_size)
    # data['val_loader'] = DataLoaderM(data['x_val'], data['y_val'], valid_batch_size)
    # data['test_loader'] = DataLoaderM(data['x_test'], data['y_test'], test_batch_size)
    # data['scaler'] = scaler
    # return data



import os
def get_dataloader(args, normalizer = 'std', tod=False, dow=False, weather=False, single=True):
    # #load raw st dataset
    # data = load_st_dataset(args.dataset)        # B, N, D
    # #normalize st data
    # data, scaler = normalize_dataset(data, normalizer, args.column_wise)
    # #spilit dataset by days or by ratio
    # if args.test_ratio > 1:
    #     data_train, data_val, data_test = split_data_by_days(data, args.val_ratio, args.test_ratio)
    # else:
    #     data_train, data_val, data_test = split_data_by_ratio(data, args.val_ratio, args.test_ratio)
    # #add time window
    # x_tra, y_tra = Add_Window_Horizon(data_train, args.lag, args.horizon, single)
    # x_val, y_val = Add_Window_Horizon(data_val, args.lag, args.horizon, single)
    # x_test, y_test = Add_Window_Horizon(data_test, args.lag, args.horizon, single)

    data = {}
    for category in ['train', 'val', 'test']:
        cat_data = np.load(os.path.join(args.dataset_dir, category + '.npz'))
        data['x_' + category] = cat_data['x']
        data['y_' + category] = cat_data['y']
    scaler = StandardScaler(mean=data['x_train'][..., 0].mean(), std=data['x_train'][..., 0].std())
    # Data format
    for category in ['train', 'val', 'test']:
        data['x_' + category][..., 0] = scaler.transform(data['x_' + category][..., 0])

    x_tra, y_tra, x_val, y_val, x_test, y_test = data['x_train'], data['y_train'], data['x_val'], data['y_val'], data['x_test'], data['y_test']


    print('Train: ', x_tra.shape, y_tra.shape)
    print('Val: ', x_val.shape, y_val.shape)
    print('Test: ', x_test.shape, y_test.shape)
    ##############get dataloader######################
    train_dataloader = data_loader(x_tra, y_tra, args.batch_size, shuffle=True, drop_last=True)
    if len(x_val) == 0:
        val_dataloader = None
    else:
        val_dataloader = data_loader(x_val, y_val, args.batch_size, shuffle=False, drop_last=True)
    test_dataloader = data_loader(x_test, y_test, args.batch_size, shuffle=False, drop_last=False)
    return train_dataloader, val_dataloader, test_dataloader, scaler

# def get_dataloader(args, normalizer = 'std', tod=False, dow=False, weather=False, single=True):
#     #load raw st dataset
#     data = load_st_dataset(args.dataset)        # B, N, D
#     #normalize st data
#     data, scaler = normalize_dataset(data, normalizer, args.column_wise)
#     #spilit dataset by days or by ratio
#     if args.test_ratio > 1:
#         data_train, data_val, data_test = split_data_by_days(data, args.val_ratio, args.test_ratio)
#     else:
#         data_train, data_val, data_test = split_data_by_ratio(data, args.val_ratio, args.test_ratio)
#     #add time window
#     x_tra, y_tra = Add_Window_Horizon(data_train, args.lag, args.horizon, single)
#     x_val, y_val = Add_Window_Horizon(data_val, args.lag, args.horizon, single)
#     x_test, y_test = Add_Window_Horizon(data_test, args.lag, args.horizon, single)
#     print('Train: ', x_tra.shape, y_tra.shape)
#     print('Val: ', x_val.shape, y_val.shape)
#     print('Test: ', x_test.shape, y_test.shape)
#     ##############get dataloader######################
#     train_dataloader = data_loader(x_tra, y_tra, args.batch_size, shuffle=True, drop_last=True)
#     if len(x_val) == 0:
#         val_dataloader = None
#     else:
#         val_dataloader = data_loader(x_val, y_val, args.batch_size, shuffle=False, drop_last=True)
#     test_dataloader = data_loader(x_test, y_test, args.batch_size, shuffle=False, drop_last=False)
#     return train_dataloader, val_dataloader, test_dataloader, scaler


if __name__ == '__main__':
    import argparse
    #MetrLA 207; BikeNYC 128; SIGIR_solar 137; SIGIR_electric 321
    DATASET = 'SIGIR_electric'
    if DATASET == 'MetrLA':
        NODE_NUM = 207
    elif DATASET == 'BikeNYC':
        NODE_NUM = 128
    elif DATASET == 'SIGIR_solar':
        NODE_NUM = 137
    elif DATASET == 'SIGIR_electric':
        NODE_NUM = 321
    parser = argparse.ArgumentParser(description='PyTorch dataloader')
    parser.add_argument('--dataset', default=DATASET, type=str)
    parser.add_argument('--num_nodes', default=NODE_NUM, type=int)
    parser.add_argument('--val_ratio', default=0.1, type=float)
    parser.add_argument('--test_ratio', default=0.2, type=float)
    parser.add_argument('--lag', default=12, type=int)
    parser.add_argument('--horizon', default=12, type=int)
    parser.add_argument('--batch_size', default=64, type=int)
    args = parser.parse_args()
    train_dataloader, val_dataloader, test_dataloader, scaler = get_dataloader(args, normalizer = 'std', tod=False, dow=False, weather=False, single=True)